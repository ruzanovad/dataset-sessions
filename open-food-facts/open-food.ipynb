{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, make_scorer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"HC-85/open-food-facts\", \"nutrition-feats-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ds[\"train\"])\n",
    "df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping `glycemic-index_100g` because it is full of NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df.drop(\"glycemic-index_100g\", axis=1).sample(1000, random_state=42)\n",
    "df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"energy_100g\"].argmax()].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have lots of NaNs, and it is a problem, definitely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_dropped\n",
    "text_columns = data.select_dtypes(include=[\"object\"]).columns\n",
    "numeric_columns = data.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "\n",
    "s = SimpleImputer(\n",
    "    missing_values=None, strategy=\"constant\", fill_value=\"\"\n",
    ").fit_transform(data.loc[data[text_columns].isnull().any(axis=1).index][text_columns])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pd = pd.DataFrame(s, columns=text_columns)\n",
    "s_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer().fit_transform(s_pd[\"product_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer().fit_transform(s_pd[\"quantity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pd[s_pd.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numeric_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNImputer(missing_values=np.nan, n_neighbors=3).fit_transform(\n",
    "    data[numeric_columns]\n",
    ")\n",
    "knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pipeline(data: pd.DataFrame, model, param_grid, verbose=100):\n",
    "    text_columns = data.select_dtypes(include=[\"object\"]).columns\n",
    "    numeric_columns = data.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "    # Define transformers for categorical and numerical features\n",
    "    numerical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", KNNImputer(missing_values=np.nan, n_neighbors=2)),  # Impute missing values\n",
    "            (\"scaler\", StandardScaler()),  # Standardize features\n",
    "        ]\n",
    "    )\n",
    "    def preprocess_text(data):\n",
    "    # Ensure data is a string, handle missing values\n",
    "        return data.astype(str)\n",
    "\n",
    "    text_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"simputer\",\n",
    "                SimpleImputer(missing_values=None, strategy=\"constant\", fill_value=\"\"),\n",
    "            ),\n",
    "            (\"function\", FunctionTransformer(preprocess_text))\n",
    "            ,\n",
    "            (\"vectorizer\", CountVectorizer()),  # Encode text features\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine transformers using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numerical_transformer, numeric_columns),\n",
    "            (\"text\", text_transformer, text_columns),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[(\"preprocessor\", preprocessor), (\"clustering\", model)], verbose=True\n",
    "    )\n",
    "    max_score = -1.\n",
    "    def silhouette_scorer(estimator, X):\n",
    "        global max_score\n",
    "        # Obtain the feature-transformed data\n",
    "        X_transformed = estimator.named_steps['preprocessor'].transform(X)\n",
    "        # Predict the cluster labels\n",
    "        cluster_labels = estimator.named_steps['clustering'].fit_predict(X_transformed)\n",
    "        # Calculate the silhouette score\n",
    "        max_score = np.max(silhouette_score(X_transformed, cluster_labels), max_score)\n",
    "        return max_score\n",
    "\n",
    "    # Set up GridSearchCV\n",
    " \n",
    "    grid_search = GridSearchCV(\n",
    "        pipe, param_grid, cv=5, scoring=silhouette_scorer, n_jobs=-1, verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(data)\n",
    "\n",
    "    # Get the best parameters and best score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    return best_params, best_score\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__n_neighbors\": [\n",
    "        1, 3,\n",
    "    ],  # Tune n_neighbors for KNNImputer\n",
    "    \"clustering__n_clusters\": [4],  # Tune number of clusters for KMeans\n",
    "    \"clustering__init\": [\"k-means++\"],  # Different initializations\n",
    "    \"clustering__max_iter\": [500, 1000],  # Max iterations for convergence\n",
    "}\n",
    "pipeline(df_dropped[numeric_columns], KMeans(random_state=42), param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
