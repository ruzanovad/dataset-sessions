{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading open-food dataset from HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"HC-85/open-food-facts\", \"nutrition-feats-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ds[\"train\"])\n",
    "df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the count of missing values in each column to assess the extent of the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping 'glycemic-index_100g' column as it contains mostly NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df.drop(\"glycemic-index_100g\", axis=1)\n",
    "df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.loc[df_dropped[\"energy_100g\"].argmax()].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing separate index variables for text cols and numeric cols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = df_dropped.select_dtypes(include=[\"object\"]).columns\n",
    "numeric_columns = df_dropped.select_dtypes(include=[\"number\"]).columns\n",
    "text_columns, numeric_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating and visualizing the correlation matrix to identify relationships between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df_dropped[numeric_columns].corr()\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 16))\n",
    "sns.heatmap(df_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying pairs of columns with a correlation of 1, indicating perfect redundancy and dropping one of each pair to avoid multicollinearity and reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_corr.abs()  # -1 is bad too\n",
    "upper_tri = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")  # matrix is symmetric\n",
    "\n",
    "threshold = 0.999\n",
    "\n",
    "# Find columns with a correlation of 1\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = df_dropped.drop(columns=to_drop)\n",
    "numeric_columns = reduced_df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "plt.figure(figsize=(30, 16))\n",
    "sns.heatmap(reduced_df[numeric_columns].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping negative values\n",
    "Dropping rows with negative values in columns where negatives are not valid. That ensures data integrity and removes potential noise or errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_columns:\n",
    "    print(col)\n",
    "    reduced_df = reduced_df[(reduced_df[col] >= 0) | (reduced_df[col].isnull())]\n",
    "    print(len(reduced_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "This will involve handling text features, numeric features, and any necessary transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text features\n",
    "This will involve handling text features, numeric features, and any necessary transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_dropped\n",
    "text_columns = data.select_dtypes(include=[\"object\"]).columns\n",
    "numeric_columns = data.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "\n",
    "s = SimpleImputer(\n",
    "    missing_values=None, strategy=\"constant\", fill_value=\"\"\n",
    ").fit_transform(data.loc[data[text_columns].isnull().any(axis=1).index][text_columns])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pd = pd.DataFrame(s, columns=text_columns)\n",
    "s_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer().fit_transform(s_pd[\"product_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer().fit_transform(s_pd[\"quantity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pd[s_pd.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric features\n",
    "Handling numeric features, which might involve scaling, normalizing, or imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numeric_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNImputer(missing_values=np.nan, n_neighbors=3).fit_transform(\n",
    "    data[numeric_columns]\n",
    ")\n",
    "knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making pipeline\n",
    "Creating a machine learning pipeline that includes preprocessing steps and the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data: pd.DataFrame, model, param_grid, verbose=100):\n",
    "    text_columns = data.select_dtypes(include=[\"object\"]).columns\n",
    "    numeric_columns = data.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "    # Define transformers for categorical and numerical features\n",
    "    numerical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"imputer\",\n",
    "                KNNImputer(missing_values=np.nan, n_neighbors=2),\n",
    "            ),  # Impute missing values\n",
    "            (\"scaler\", StandardScaler()),  # Standardize features\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def preprocess_text(data):\n",
    "        # Ensure data is a string, handle missing values\n",
    "        return data.astype(str)\n",
    "\n",
    "    text_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                \"simputer\",\n",
    "                SimpleImputer(missing_values=None, strategy=\"constant\", fill_value=\"\"),\n",
    "            ),\n",
    "            (\"function\", FunctionTransformer(preprocess_text)),\n",
    "            (\"vectorizer\", CountVectorizer()),  # Encode text features\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine transformers using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numerical_transformer, numeric_columns),\n",
    "            (\"text\", text_transformer, text_columns),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[(\"preprocessor\", preprocessor), (\"clustering\", model)], verbose=True\n",
    "    )\n",
    "    max_score = -1.0\n",
    "\n",
    "    def silhouette_scorer(estimator, X):\n",
    "        global max_score\n",
    "        # Obtain the feature-transformed data\n",
    "        X_transformed = estimator.named_steps[\"preprocessor\"].transform(X)\n",
    "        # Predict the cluster labels\n",
    "        cluster_labels = estimator.named_steps[\"clustering\"].fit_predict(X_transformed)\n",
    "        # Calculate the silhouette score\n",
    "        max_score = np.max(silhouette_score(X_transformed, cluster_labels), max_score)\n",
    "        return max_score\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        pipe, param_grid, cv=5, scoring=silhouette_scorer, n_jobs=-1, verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(data)\n",
    "\n",
    "    # Get the best parameters and best score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    return best_params, best_score\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__n_neighbors\": [\n",
    "        1,\n",
    "        3,\n",
    "    ],  # Tune n_neighbors for KNNImputer\n",
    "    \"clustering__n_clusters\": [4],  # Tune number of clusters for KMeans\n",
    "    \"clustering__init\": [\"k-means++\"],  # Different initializations\n",
    "    \"clustering__max_iter\": [500, 1000],  # Max iterations for convergence\n",
    "}\n",
    "pipeline(df_dropped[numeric_columns], KMeans(random_state=42), param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
