{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "sns.set_style(\"dark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p data\n",
    "# %pip install kaggle\n",
    "# !kaggle datasets download -d parisrohan/credit-score-classification -p data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -u data/credit-score-classification.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\", decimal=\".\", engine=\"python\") \\\n",
    ".sample(1000, random_state=42) # working on a slow laptop\n",
    "test_df = pd.read_csv(\"data/test.csv\", decimal=\".\", engine=\"python\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Objective: Start by getting a basic understanding of the dataset. This includes checking the data types of columns, identifying missing values (NaNs), and understanding the overall structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique clients do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"ID\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying columns that are incorrectly typed (e.g., numerical data stored as strings) and casting them to the correct data types. This is important for accurate analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_columns_float = [\n",
    "    \"Annual_Income\",\n",
    "    \"Changed_Credit_Limit\",\n",
    "    \"Outstanding_Debt\",\n",
    "    \"Total_EMI_per_month\",\n",
    "    \"Amount_invested_monthly\",\n",
    "    \"Monthly_Balance\",\n",
    "]\n",
    "train_df[problem_columns_float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[problem_columns_float] = train_df[problem_columns_float].apply(\n",
    "    pd.to_numeric, errors=\"coerce\"\n",
    ")\n",
    "train_df[problem_columns_float] = train_df[problem_columns_float].astype(\"float64\")\n",
    "train_df.loc[:, problem_columns_float].fillna(\n",
    "    value=train_df[problem_columns_float].median(),\n",
    "    inplace=True,\n",
    ")\n",
    "train_df[problem_columns_float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_columns_int = [\"Num_of_Loan\", \"Num_of_Delayed_Payment\", \"Age\"]\n",
    "train_df[problem_columns_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[problem_columns_int] = train_df[problem_columns_int].apply(\n",
    "    pd.to_numeric, errors=\"coerce\"\n",
    ")\n",
    "train_df[problem_columns_int] = train_df[problem_columns_int].astype(pd.Int32Dtype())\n",
    "train_df.loc[:, problem_columns_int].fillna(\n",
    "    value=train_df[problem_columns_int].median(), inplace=True\n",
    ")\n",
    "train_df[problem_columns_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare separate variables for different column types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_columns = train_df.select_dtypes(\"number\").columns\n",
    "categorical_columns = train_df.select_dtypes(\"object\").columns.drop(\"Credit_Score\")\n",
    "feature_columns = categorical_columns.union(number_columns)\n",
    "target = train_df[\"Credit_Score\"]\n",
    "number_columns, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify and address outliers in the dataset, such as negative values where they don't make sense (e.g., age cannot be negative), or unrealistic values like an age of 8698."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(\n",
    "    train_df[\n",
    "        (train_df[\"Age\"] < 0)\n",
    "        | (train_df[\"Age\"] > 100)\n",
    "        | (train_df[\"Num_Bank_Accounts\"] < 0)\n",
    "        | (train_df[\"Num_of_Loan\"] < 0)\n",
    "        | (train_df[\"Num_of_Delayed_Payment\"] < 0)\n",
    "        | (train_df[\"Delay_from_due_date\"] < 0)\n",
    "    ].index\n",
    ")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various plots\n",
    "Using visualizations like box plots and pie charts to get insights into the distribution of data, detect outliers, and understand categorical distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_data = train_df[\n",
    "    number_columns\n",
    "]\n",
    "\n",
    "nrows = 4\n",
    "\n",
    "fig, axes = plt.subplots(nrows, len(number_columns)//nrows, figsize=(15, 10))\n",
    "\n",
    "for i, column in enumerate(box_data.columns):\n",
    "    row = i // (len(number_columns) // nrows)  \n",
    "    col = i % (len(number_columns) // nrows) \n",
    "\n",
    "\n",
    "    non_nan_data = box_data[column].dropna()\n",
    "    axes[row, col].boxplot(non_nan_data, vert=False)\n",
    "    axes[row, col].set_xlabel(column)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This box plot shows that there are many outliers (points outside the whiskers), indicating possible data issues or extreme variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\n",
    "    [\n",
    "        \"Month\",\n",
    "        \"Occupation\",\n",
    "        \"Type_of_Loan\",\n",
    "        \"Credit_Mix\",\n",
    "        \"Payment_of_Min_Amount\",\n",
    "        \"Payment_Behaviour\",\n",
    "        \"Credit_Score\",\n",
    "    ]\n",
    "].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_data = train_df[\n",
    "    [\n",
    "        \"Month\",\n",
    "        \"Occupation\",\n",
    "        \"Credit_Mix\",\n",
    "        \"Payment_of_Min_Amount\",\n",
    "        \"Payment_Behaviour\",\n",
    "        \"Credit_Score\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "nrows = 2\n",
    "ncols = 3  \n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(15, 8))\n",
    "\n",
    "for i, column in enumerate(pie_data.columns):\n",
    "    row = i // ncols\n",
    "    col = i % ncols\n",
    "\n",
    "    grouped_data = pie_data.groupby(column).size().reset_index(name=\"counts\")\n",
    "\n",
    "    # Extract data for the pie chart\n",
    "    labels = grouped_data[column]\n",
    "    sizes = grouped_data[\"counts\"]\n",
    "\n",
    "    # Plot pie chart in the correct subplot\n",
    "    axes[row, col].pie(sizes, labels=labels, autopct=\"%1.1f%%\", startangle=90)\n",
    "    axes[row, col].set_xlabel(column)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pie chart shows the distribution of categories within 'categorical_column'. It reveals that some categories dominate the dataset, while others are underrepresented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Donut charts (nested pie charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_data = train_df[\n",
    "    [\n",
    "        \"Month\",\n",
    "        \"Occupation\",\n",
    "        \"Credit_Mix\",\n",
    "        \"Payment_of_Min_Amount\",\n",
    "        \"Payment_Behaviour\",\n",
    "        \"Credit_Score\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Group by 'Credit_Score' for the inner layer\n",
    "inner_group = pie_data.groupby('Credit_Score').size().reset_index(name='counts')\n",
    "inner_labels = inner_group['Credit_Score']\n",
    "inner_sizes = inner_group['counts']\n",
    "\n",
    "# Define the outer layers (columns other than 'Credit_Score')\n",
    "outer_columns = [\"Month\", \"Occupation\", \"Credit_Mix\", \"Payment_of_Min_Amount\", \"Payment_Behaviour\"]\n",
    "\n",
    "nrows = 2\n",
    "ncols = 3\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(25, 10))\n",
    "\n",
    "for i, column in enumerate(outer_columns):\n",
    "    outer_group = pie_data.groupby(['Credit_Score', column]).size().reset_index(name='counts')\n",
    "    \n",
    "    row = i // ncols\n",
    "    col = i % ncols\n",
    "\n",
    "    # Create the outer labels and sizes based on the groups\n",
    "    outer_labels = outer_group[column]\n",
    "    outer_sizes = outer_group['counts']\n",
    "    \n",
    "    # Create the donut chart\n",
    "    axes[row, col].pie(inner_sizes, labels=inner_labels, radius=1, wedgeprops=dict(width=0.3, edgecolor='w'), autopct=\"%1.1f%%\", startangle=90)\n",
    "    axes[row, col].pie(outer_sizes, labels=outer_labels, radius=1.3, wedgeprops=dict(width=0.3, edgecolor='w'), autopct=\"%1.1f%%\", startangle=90)\n",
    "    \n",
    "    # Add title for the outer layer\n",
    "    axes[row, col].set_title(column)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing Values & Feature Extraction\n",
    "**Objective**: Identify columns with missing values and decide how to handle them (e.g., drop, fill, or impute). Also, extract new features from existing data to enhance the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding nulls in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Name' column is not crucial for our analysis since we have 'ID' as a unique identifier. We can safely drop the 'Name' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_col(cols):\n",
    "    \"\"\"\n",
    "    Drops a specified column from the DataFrame and its associated index variable.\n",
    "    \"\"\"\n",
    "    global train_df, categorical_columns, number_columns\n",
    "    train_df.drop(columns=cols, inplace=True)\n",
    "\n",
    "    for col in cols:\n",
    "        if col in categorical_columns:\n",
    "            categorical_columns = categorical_columns.drop(col)\n",
    "        else:\n",
    "            number_columns = number_columns.drop(col)\n",
    "\n",
    "\n",
    "delete_col([\"Name\", \"Customer_ID\", \"SSN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although usage of `nonlocal` is not recommended, in this case it is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Credit_History_Age\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming `Credit_History_Age`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Credit_History_Age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the `Credit_History_Age` column to a int format to facilitate analysis and avoid creating too many columns during one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_credit_history = train_df[\"Credit_History_Age\"].str.extract(\n",
    "    r\"(\\d+)\\sYears\\sand\\s(\\d+)\\sMonths\"\n",
    ")\n",
    "\n",
    "total_months = split_credit_history[0].astype(\n",
    "    pd.Int32Dtype()\n",
    ") * 12 + split_credit_history[1].astype(pd.Int32Dtype())\n",
    "\n",
    "train_df[\"Credit_History_Age\"] = total_months\n",
    "total_months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling `Type_of_Loan`\n",
    "By splitting and exploding 'Type_of_Loan', we can handle cases where a single individual has multiple loans, making the data more granular and accurate for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Type_of_Loan\"].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_types = [\n",
    "    \"Not Specified\",\n",
    "    \"Credit-Builder Loan\",\n",
    "    \"Personal Loan\",\n",
    "    \"Debt Consolidation Loan\",\n",
    "    \"Student Loan\",\n",
    "    \"Payday Loan\",\n",
    "    \"Mortgage Loan\",\n",
    "    \"Auto Loan\",\n",
    "    \"Home Equity Loan\",\n",
    "]\n",
    "\n",
    "train_df[\"Type_of_Loan\"].fillna(\"\", inplace=True)\n",
    "for suffix in loan_types:\n",
    "    train_df[\"Type_of_Loan_\" + suffix] = train_df[\"Type_of_Loan\"].apply(\n",
    "        lambda x: suffix in x.split(\", \")\n",
    "    )\n",
    "\n",
    "delete_col([\"Type_of_Loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = categorical_columns.drop(\"Credit_History_Age\")\n",
    "number_columns = number_columns.append(pd.Index([\"Credit_History_Age\"]))\n",
    "number_columns, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(\n",
    "    train_df,\n",
    "    columns=[\n",
    "        \"Month\",\n",
    "        \"Occupation\",\n",
    "        \"Credit_Mix\",\n",
    "        \"Payment_of_Min_Amount\",\n",
    "        \"Payment_Behaviour\",\n",
    "    ],\n",
    "    drop_first=True,\n",
    ")\n",
    "train_df = pd.get_dummies(train_df, columns=[\"Credit_Score\"])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "id_column = train_df['ID']\n",
    "features = train_df.drop(columns=['ID'])\n",
    "\n",
    "knn_imputer = KNNImputer(\n",
    "    n_neighbors=1\n",
    ")  # at least 1 neighbor, because running one cell for 7 minutes is too harsh\n",
    "# knn_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "\n",
    "X_knn_imputed = knn_imputer.fit_transform(features)\n",
    "X_knn_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_knn_imputed = pd.DataFrame(X_knn_imputed, columns=features.columns, index=features.index).astype(features.dtypes.to_dict())\n",
    "result_df = train_df_knn_imputed.copy()\n",
    "# result_df[\"ID\"] = id_column.reset_index(drop=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_knn_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "Creating a simple baseline model before diving into complex modeling. This helps to understand the minimum performance we can expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = train_df_knn_imputed.drop(\n",
    "        columns=[\"Credit_Score_Good\", \"Credit_Score_Standard\", \"Credit_Score_Poor\"]\n",
    "    )\n",
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    preprocessed_df,\n",
    "    target[preprocessed_df.index],\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "dummy_classifier = DummyClassifier(random_state=42)\n",
    "dummy_classifier.fit(X_train, y_train)\n",
    "dummy_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "balanced_accuracy_score(y_test, dummy_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving preprocessed data\n",
    "Jupyter provides a %store magic command to pass variables between notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store X_train X_test y_train y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
